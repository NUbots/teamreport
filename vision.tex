\section{Vision}
The Nao vision system followed the process: \emph{Input image, classify image, form blobs, sort colour blobs, combine soft blobs, find ball, find goals, detect lines}. Even though the vision process was based on AIBO version of NUbot code the only section that could be reused was blob formation. 

The 2009 vision module was a slight advancement over the 2008 module. This section details the main functions of the vision system and the reasons behind them. 
\subsection{Overview}

Webots was used to develop the base vision system. Inputting the RGB image and classifying it using a perfect look-up table (LUT) allowed blob formation and basic object recognition to be tested which then allowed for localisation development to begin.

We then worked on developing the vision debug application \emph{Eye of the NUbot} (EOTN). What made this application different is the HSI classification method. Transforming the YUV image to HSI values and selecting the classification region in HSI allowed for an intuitive classification method and better separation of hue ranges. 

Once we had a robot we were able to select camera settings using \emph{Telepathe} and save image streams. We could then work on object recognition.

Advancements that were included in the vision module this year were more ratio checks for the ball and goals, including a green horizon. This was to reduce the number of false objects recognised by the robots. We also included heuristics for the detection of new field objects, such as penalty spots, and the centre circle. These new field objects greatly enhanced the performance of the localisation module. Also, we increased the camera resolution from the minimal of 160x120 to an medium image of 320x240. This increased the accuracy of the object seen (example, a ball can be seen at the full length of the field), with a trade-off to robots processing speed.

\input{classandblobs.tex}
\input{objects.tex}
\input{lines.tex}